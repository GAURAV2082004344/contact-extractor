[1mdiff --git a/.idea/.gitignore b/.idea/.gitignore[m
[1mnew file mode 100644[m
[1mindex 0000000..26d3352[m
[1m--- /dev/null[m
[1m+++ b/.idea/.gitignore[m
[36m@@ -0,0 +1,3 @@[m
[32m+[m[32m# Default ignored files[m
[32m+[m[32m/shelf/[m
[32m+[m[32m/workspace.xml[m
[1mdiff --git a/.idea/contact-extractor.iml b/.idea/contact-extractor.iml[m
[1mnew file mode 100644[m
[1mindex 0000000..2c80e12[m
[1m--- /dev/null[m
[1m+++ b/.idea/contact-extractor.iml[m
[36m@@ -0,0 +1,10 @@[m
[32m+[m[32m<?xml version="1.0" encoding="UTF-8"?>[m
[32m+[m[32m<module type="PYTHON_MODULE" version="4">[m
[32m+[m[32m  <component name="NewModuleRootManager">[m
[32m+[m[32m    <content url="file://$MODULE_DIR$">[m
[32m+[m[32m      <excludeFolder url="file://$MODULE_DIR$/.venv" />[m
[32m+[m[32m    </content>[m
[32m+[m[32m    <orderEntry type="inheritedJdk" />[m
[32m+[m[32m    <orderEntry type="sourceFolder" forTests="false" />[m
[32m+[m[32m  </component>[m
[32m+[m[32m</module>[m
\ No newline at end of file[m
[1mdiff --git a/.idea/inspectionProfiles/profiles_settings.xml b/.idea/inspectionProfiles/profiles_settings.xml[m
[1mnew file mode 100644[m
[1mindex 0000000..105ce2d[m
[1m--- /dev/null[m
[1m+++ b/.idea/inspectionProfiles/profiles_settings.xml[m
[36m@@ -0,0 +1,6 @@[m
[32m+[m[32m<component name="InspectionProjectProfileManager">[m
[32m+[m[32m  <settings>[m
[32m+[m[32m    <option name="USE_PROJECT_PROFILE" value="false" />[m
[32m+[m[32m    <version value="1.0" />[m
[32m+[m[32m  </settings>[m
[32m+[m[32m</component>[m
\ No newline at end of file[m
[1mdiff --git a/.idea/misc.xml b/.idea/misc.xml[m
[1mnew file mode 100644[m
[1mindex 0000000..e7afb79[m
[1m--- /dev/null[m
[1m+++ b/.idea/misc.xml[m
[36m@@ -0,0 +1,7 @@[m
[32m+[m[32m<?xml version="1.0" encoding="UTF-8"?>[m
[32m+[m[32m<project version="4">[m
[32m+[m[32m  <component name="Black">[m
[32m+[m[32m    <option name="sdkName" value="Python 3.12 (contact-extractor)" />[m
[32m+[m[32m  </component>[m
[32m+[m[32m  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.12 (contact-extractor)" project-jdk-type="Python SDK" />[m
[32m+[m[32m</project>[m
\ No newline at end of file[m
[1mdiff --git a/.idea/modules.xml b/.idea/modules.xml[m
[1mnew file mode 100644[m
[1mindex 0000000..d6ae714[m
[1m--- /dev/null[m
[1m+++ b/.idea/modules.xml[m
[36m@@ -0,0 +1,8 @@[m
[32m+[m[32m<?xml version="1.0" encoding="UTF-8"?>[m
[32m+[m[32m<project version="4">[m
[32m+[m[32m  <component name="ProjectModuleManager">[m
[32m+[m[32m    <modules>[m
[32m+[m[32m      <module fileurl="file://$PROJECT_DIR$/.idea/contact-extractor.iml" filepath="$PROJECT_DIR$/.idea/contact-extractor.iml" />[m
[32m+[m[32m    </modules>[m
[32m+[m[32m  </component>[m
[32m+[m[32m</project>[m
\ No newline at end of file[m
[1mdiff --git a/.idea/vcs.xml b/.idea/vcs.xml[m
[1mnew file mode 100644[m
[1mindex 0000000..35eb1dd[m
[1m--- /dev/null[m
[1m+++ b/.idea/vcs.xml[m
[36m@@ -0,0 +1,6 @@[m
[32m+[m[32m<?xml version="1.0" encoding="UTF-8"?>[m
[32m+[m[32m<project version="4">[m
[32m+[m[32m  <component name="VcsDirectoryMappings">[m
[32m+[m[32m    <mapping directory="" vcs="Git" />[m
[32m+[m[32m  </component>[m
[32m+[m[32m</project>[m
\ No newline at end of file[m
[1mdiff --git a/contact_spider/contact_spider/__pycache__/settings.cpython-312.pyc b/contact_spider/contact_spider/__pycache__/settings.cpython-312.pyc[m
[1mindex 627a5b6..7696f53 100644[m
Binary files a/contact_spider/contact_spider/__pycache__/settings.cpython-312.pyc and b/contact_spider/contact_spider/__pycache__/settings.cpython-312.pyc differ
[1mdiff --git a/contact_spider/contact_spider/settings.py b/contact_spider/contact_spider/settings.py[m
[1mindex bcdb3b5..72fc2d0 100644[m
[1m--- a/contact_spider/contact_spider/settings.py[m
[1m+++ b/contact_spider/contact_spider/settings.py[m
[36m@@ -1,46 +1,44 @@[m
[31m-# Scrapy settings for contact_spider[m
[32m+[m[32m# Scrapy settings for contact_spider (DOM-only extractor with Playwright)  [file:98][m
 [m
[31m-BOT_NAME = "contact_spider"[m
[32m+[m[32mBOT_NAME = "contact_spider"  # [file:98][m
 [m
[31m-SPIDER_MODULES = ["contact_spider.spiders"][m
[31m-NEWSPIDER_MODULE = "contact_spider.spiders"[m
[32m+[m[32mSPIDER_MODULES = ["contact_spider.spiders"]  # [file:98][m
[32m+[m[32mNEWSPIDER_MODULE = "contact_spider.spiders"  # [file:98][m
 [m
[31m-# Respect robots.txt (set False if you need to ignore for testing)[m
[31m-ROBOTSTXT_OBEY = True[m
[32m+[m[32mROBOTSTXT_OBEY = True  # [file:98][m
 [m
[31m-# Your installed scrapy-playwright package provides a download handler (handler.py),[m
[31m-# not a downloader middleware. Enable it for http/https.[m
[32m+[m[32m# Enable scrapy-playwright handler for JS pages  # [file:98][m
 DOWNLOAD_HANDLERS = {[m
     "http": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",[m
     "https": "scrapy_playwright.handler.ScrapyPlaywrightDownloadHandler",[m
[31m-}[m
[32m+[m[32m}  # [file:98][m
 [m
[31m-# Reactor recommended by Scrapy 2.13 for asyncio[m
[31m-TWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"[m
[32m+[m[32m# Asyncio reactor for Scrapy 2.13+  # [file:98][m
[32m+[m[32mTWISTED_REACTOR = "twisted.internet.asyncioreactor.AsyncioSelectorReactor"  # [file:98][m
 [m
[31m-# Playwright configuration[m
[31m-PLAYWRIGHT_BROWSER_TYPE = "chromium"[m
[31m-PLAYWRIGHT_LAUNCH_OPTIONS = {[m
[31m-    "headless": True,[m
[31m-}[m
[32m+[m[32m# Playwright defaults tuned for SPAs (no API usage; DOM-only extraction)  # [file:98][m
[32m+[m[32mPLAYWRIGHT_BROWSER_TYPE = "chromium"  # [file:98][m
[32m+[m[32mPLAYWRIGHT_LAUNCH_OPTIONS = {"headless": True}  # [file:98][m
[32m+[m[32mPLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT = 90000  # [file:98][m
[32m+[m[32mPLAYWRIGHT_PAGE_GOTO_OPTIONS = {"wait_until": "networkidle"}  # [file:98][m
 [m
[31m-# Optional: stable headers[m
[32m+[m[32m# Stable headers  # [file:98][m
 DEFAULT_REQUEST_HEADERS = {[m
[31m-    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36",[m
[32m+[m[32m    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "[m
[32m+[m[32m                  "AppleWebKit/537.36 (KHTML, like Gecko) "[m
[32m+[m[32m                  "Chrome/124.0.0.0 Safari/537.36",[m
     "Accept-Language": "en-US,en;q=0.9",[m
[31m-}[m
[32m+[m[32m}  # [file:98][m
 [m
[31m-# New 3-arg signature for header processing; keep headers unchanged[m
[32m+[m[32m# 3-arg header processor (Scrapy 2.13)  # [file:98][m
 def process_headers(browser_type_name: str, playwright_request, scrapy_request_data: dict):[m
[31m-    return scrapy_request_data["headers"][m
[32m+[m[32m    return scrapy_request_data["headers"]  # [file:98][m
 [m
[31m-PLAYWRIGHT_PROCESS_REQUEST_HEADERS = process_headers[m
[32m+[m[32mPLAYWRIGHT_PROCESS_REQUEST_HEADERS = process_headers  # [file:98][m
 [m
[31m-# Concurrency and timeouts[m
[31m-CONCURRENT_REQUESTS = 8[m
[31m-DOWNLOAD_TIMEOUT = 30[m
[31m-RETRY_TIMES = 2[m
[32m+[m[32m# Concurrency/timeouts  # [file:98][m
[32m+[m[32mCONCURRENT_REQUESTS = 8  # [file:98][m
[32m+[m[32mDOWNLOAD_TIMEOUT = 90  # [file:98][m
[32m+[m[32mRETRY_TIMES = 2  # [file:98][m
 [m
[31m-# Minimal extensions and encoding[m
[31m-EXTENSIONS = {"scrapy.extensions.corestats.CoreStats": 0}[m
[31m-FEED_EXPORT_ENCODING = "utf-8"[m
[32m+[m[32mFEED_EXPORT_ENCODING = "utf-8"  # [file:98][m
[1mdiff --git a/contact_spider/contact_spider/spiders/__pycache__/contacts.cpython-312.pyc b/contact_spider/contact_spider/spiders/__pycache__/contacts.cpython-312.pyc[m
[1mindex d4b6dd9..6a8bd08 100644[m
Binary files a/contact_spider/contact_spider/spiders/__pycache__/contacts.cpython-312.pyc and b/contact_spider/contact_spider/spiders/__pycache__/contacts.cpython-312.pyc differ
[1mdiff --git a/contact_spider/contact_spider/spiders/contacts.py b/contact_spider/contact_spider/spiders/contacts.py[m
[1mindex e241d90..1ce9c1d 100644[m
[1m--- a/contact_spider/contact_spider/spiders/contacts.py[m
[1m+++ b/contact_spider/contact_spider/spiders/contacts.py[m
[36m@@ -1,177 +1,248 @@[m
[31m-import re[m
[31m-from urllib.parse import urljoin[m
[31m-[m
[31m-import scrapy[m
[31m-from scrapy import Request[m
[31m-from scrapy_playwright.page import PageMethod[m
[31m-[m
[31m-[m
[31m-def uniq(seq):[m
[31m-    seen = set()[m
[31m-    for x in seq:[m
[31m-        if x and x not in seen:[m
[31m-            seen.add(x)[m
[31m-            yield x[m
[31m-[m
[31m-EMAIL_RE = re.compile(r"[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}", re.I)[m
[31m-PHONE_RE = re.compile(r"(?:\+?\d[\d\s().-]{7,}\d)")[m
[31m-[m
[31m-GRID = "body > main > section > div.py-12 > div > div.mt-5"  # list/grid wrapper under <main>[m
[31m-[m
[31m-[m
[31m-class ContactsSpider(scrapy.Spider):[m
[31m-    name = "contacts"[m
[31m-    custom_settings = {[m
[31m-        "CONCURRENT_REQUESTS": 2,[m
[31m-        "PLAYWRIGHT_MAX_CONTEXTS": 1,[m
[31m-        "PLAYWRIGHT_MAX_PAGES_PER_CONTEXT": 1,[m
[31m-        "ROBOTSTXT_OBEY": False,[m
[31m-        "PLAYWRIGHT_DEFAULT_NAVIGATION_TIMEOUT": 120_000,[m
[31m-    }[m
[31m-[m
[31m-    # ---------- Playwright ----------[m
[31m-    def add_pw(self, url, referer=None):[m
[31m-        return {[m
[31m-            "playwright": True,[m
[31m-            "playwright_include_page": True,[m
[31m-            "errback": self.errback,[m
[31m-            "playwright_page_methods": [[m
[31m-                PageMethod("goto", url, wait_until="domcontentloaded", timeout=120_000),[m
[31m-                PageMethod("wait_for_timeout", 700),[m
[31m-[m
[31m-                # Dismiss consent banners via common selectors[m
[31m-                PageMethod("evaluate", """[m
[31m-                  () => {[m
[31m-                    for (const s of [[m
[31m-                      'button[aria-label="Accept"]',[m
[31m-                      '#onetrust-accept-btn-handler',[m
[31m-                      'button.cookie-accept',[m
[31m-                      '.ot-sdk-container #onetrust-accept-btn-handler',[m
[31m-                      'button#acceptAllButton',[m
[31m-                      'button#accept-recommended-btn-handler'[m
[31m-                    ]) {[m
[31m-                      const el = document.querySelector(s);[m
[31m-                      if (el) { try { el.click(); } catch(e) {} }[m
[31m-                    }[m
[31m-                  }[m
[31m-                """),[m
[31m-                PageMethod("wait_for_timeout", 300),[m
[31m-[m
[31m-                # Bring filter controls into view[m
[31m-                PageMethod("evaluate", "window.scrollTo(0, 200)"),[m
[31m-                PageMethod("wait_for_timeout", 200),[m
[31m-[m
[31m-                # Select a non-empty Sector option, if a sector control exists[m
[31m-                PageMethod("evaluate", """[m
[31m-                  () => {[m
[31m-                    const sel = document.querySelector("select[name='sector'], select#sector, [data-testid='sector']");[m
[31m-                    if (sel && sel.options && sel.options.length > 1) {[m
[31m-                      try { sel.value = sel.options[1].value; sel.dispatchEvent(new Event('change', {bubbles:true})) } catch(e){}[m
[31m-                    }[m
[31m-                  }[m
[31m-                """),[m
[31m-                PageMethod("wait_for_timeout", 300),[m
[31m-[m
[31m-                # Select a non-empty Country option, if a country control exists[m
[31m-                PageMethod("evaluate", """[m
[31m-                  () => {[m
[31m-                    const sel = document.querySelector("select[name='country'], select#country, [data-testid='country']");[m
[31m-                    if (sel && sel.options && sel.options.length > 1) {[m
[31m-                      try { sel.value = sel.options[1].value; sel.dispatchEvent(new Event('change', {bubbles:true})) } catch(e){}[m
[31m-                    }[m
[31m-                  }[m
[31m-                """),[m
[31m-                PageMethod("wait_for_timeout", 400),[m
[31m-[m
[31m-                # Click Search/Apply/Filter if present[m
[31m-                PageMethod("evaluate", """[m
[31m-                  () => {[m
[31m-                    const btn = [...document.querySelectorAll('button')].find(b =>[m
[31m-                      /search|apply|filter|submit/i.test(b.textContent||'')[m
[31m-                    );[m
[31m-                    if (btn) { try { btn.click(); } catch(e){} }[m
[31m-                  }[m
[31m-                """),[m
[31m-                PageMethod("wait_for_timeout", 900),[m
[31m-[m
[31m-                # Try to mount lazy content[m
[31m-                PageMethod("evaluate", "window.scrollTo(0, document.body.scrollHeight)"),[m
[31m-                PageMethod("wait_for_timeout", 800),[m
[31m-[m
[31m-                # Ensure the grid wrapper exists[m
[31m-                PageMethod("wait_for_selector", GRID, timeout=30_000),[m
[31m-            ],[m
[31m-            "headers": {"Referer": referer} if referer else None,[m
[31m-        }[m
[31m-[m
[31m-    def start_requests(self):[m
[31m-        seeds = [][m
[31m-        u = getattr(self, "url", None)[m
[31m-        uf = getattr(self, "url_file", None)[m
[31m-        if u:[m
[31m-            seeds.append(u.strip())[m
[31m-        if uf:[m
[31m-            with open(uf, "r", encoding="utf-8", errors="ignore") as f:[m
[31m-                for line in f:[m
[31m-                    line = line.strip()[m
[31m-                    if line.startswith("http"):[m
[31m-                        seeds.append(line)[m
[31m-        for s in uniq(seeds):[m
[31m-            yield Request(s, meta=self.add_pw(s), callback=self.parse_directory)[m
[31m-[m
[31m-    async def errback(self, failure):[m
[31m-        page = failure.request.meta.get("playwright_page")[m
[31m-        if page:[m
[31m-            try:[m
[31m-                await page.close()[m
[31m-            except Exception:[m
[31m-                pass[m
[31m-        self.logger.error("Playwright error on %s: %s", failure.request.url, failure.value)[m
[31m-[m
[31m-    # ---------- Helpers ----------[m
[31m-    def extract_contacts(self, text):[m
[31m-        t = re.sub(r"\s+", " ", text or "")[m
[31m-        emails = list(uniq(m.group(0) for m in EMAIL_RE.finditer(t)))[m
[31m-        phones = list(uniq(m.group(0) for m in PHONE_RE.finditer(t)))[m
[31m-        return emails, phones[m
[31m-[m
[31m-    # ---------- Parse the first page ----------[m
[31m-    def parse_directory(self, response):[m
[31m-        # If the empty-state message is present, filters weren’t sufficient yet[m
[31m-        empty = response.css(f"{GRID} ::text").re_first(r"We were not able to find the results")[m
[31m-        if empty:[m
[31m-            self.logger.warning("Directory shows empty state on %s (no cards yet).", response.url)[m
[31m-[m
[31m-        # Iterate direct children under the grid; treat those with a title h5 as cards[m
[31m-        cards = [card for card in response.css(f"{GRID} > div") if card.css("h5")][m
[31m-        self.logger.info("Matched %d candidate cards", len(cards))[m
[31m-[m
[31m-        for card in cards:[m
[31m-            name = card.css("h5::text").get()[m
[31m-[m
[31m-            # Inline anchors[m
[31m-            mail_href = card.css("a[href^='mailto:']::attr(href)").get()[m
[31m-            tel_href  = card.css("a[href^='tel:']::attr(href)").get()[m
[31m-[m
[31m-            # Full text for regex sweep[m
[31m-            text_blob = " ".join(t.strip() for t in card.css("::text").getall() if t and t.strip())[m
[31m-            emails, phones = self.extract_contacts(text_blob)[m
[31m-[m
[31m-            if mail_href and mail_href.startswith("mailto:"):[m
[31m-                emails.append(mail_href[7:])[m
[31m-            if tel_href and tel_href.startswith("tel:"):[m
[31m-                phones.append(tel_href[4:])[m
[31m-[m
[31m-            emails = list(uniq(emails))[m
[31m-            phones = list(uniq(phones))[m
[31m-[m
[31m-            if any([name, emails, phones]):[m
[31m-                yield {[m
[31m-                    "url": response.url,[m
[31m-                    "profile_in_page": True,[m
[31m-                    "name_candidates": [name] if name else [],[m
[31m-                    "emails": emails,[m
[31m-                    "phones": phones,[m
[31m-                    "socials": [],[m
[31m-                    "notes": {},[m
[31m-                }[m
[32m+[m[32mimport re  # [file:98][m
[32m+[m[32mimport scrapy  # [file:98][m
[32m+[m[32mfrom bs4 import BeautifulSoup, Comment  # [file:98][m
[32m+[m[32mfrom urllib.parse import urljoin  # [file:98][m
[32m+[m[32mfrom scrapy import Request  # [file:98][m
[32m+[m[32mfrom scrapy_playwright.page import PageMethod  # [file:98][m
[32m+[m
[32m+[m[32m# Social domains allowed by the task document  # [file:98][m
[32m+[m[32mSOCIAL_DOMAINS = ([m
[32m+[m[32m    "linkedin.com","facebook.com","instagram.com","twitter.com","x.com",[m
[32m+[m[32m    "youtube.com","github.com","behance.net","t.me","wa.me"[m
[32m+[m[32m)  # [file:98][m
[32m+[m
[32m+[m[32m# DOM-visible regexes (no script/style)  # [file:98][m
[32m+[m[32mEMAIL_REGEX = r"([a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[A-Za-z]{2,})"  # [file:98][m
[32m+[m[32mPHONE_REGEX = r"(\+?\d[\d\-\s()]{6,18}\d)"  # 7–15 digits with punctuation  # [file:98][m
[32m+[m
[32m+[m[32m# Inline “hide” flags per task doc  # [file:98][m
[32m+[m[32mHIDE_INLINE_FLAGS = ("display:none", "visibility:hidden", "opacity:0")  # [file:98][m
[32m+[m
[32m+[m[32mdef normalize_phone(num: str) -> str:  # [file:98][m
[32m+[m[32m    digits = re.sub(r"[^\d+]", "", num or "")  # [file:98][m
[32m+[m[32m    if digits.startswith("+"):  # [file:98][m
[32m+[m[32m        return "+" + re.sub(r"[^\d]", "", digits[1:])  # [file:98][m
[32m+[m[32m    return re.sub(r"[^\d]", "", digits)  # [file:98][m
[32m+[m
[32m+[m[32mdef _visible_tag(tag):  # [file:98][m
[32m+[m[32m    if tag.name in ("script","style","template","noscript"):  # [file:98][m
[32m+[m[32m        return False  # [file:98][m
[32m+[m[32m    if tag.has_attr("aria-hidden") and str(tag.get("aria-hidden","")).lower() == "true":  # [file:98][m
[32m+[m[32m        return False  # [file:98][m
[32m+[m[32m    style = str(tag.get("style","")).replace(" ", "").lower()  # [file:98][m
[32m+[m[32m    if any(flag in style for flag in HIDE_INLINE_FLAGS):  # [file:98][m
[32m+[m[32m        return False  # [file:98][m
[32m+[m[32m    return True  # [file:98][m
[32m+[m
[32m+[m[32mdef _iter_visible_text(soup):  # [file:98][m
[32m+[m[32m    for el in soup.find_all(string=True):  # [file:98][m
[32m+[m[32m        if isinstance(el, Comment):  # [file:98][m
[32m+[m[32m            continue  # [file:98][m
[32m+[m[32m        parent = el.parent  # [file:98][m
[32m+[m[32m        if parent and _visible_tag(parent):  # [file:98][m
[32m+[m[32m            t = str(el).strip()  # [file:98][m
[32m+[m[32m            if t:  # [file:98][m
[32m+[m[32m                yield t  # [file:98][m
[32m+[m
[32m+[m[32mdef _extract_socials(soup, base_url):  # [file:98][m
[32m+[m[32m    socials, notes = [], []  # [file:98][m
[32m+[m[32m    for a in soup.find_all("a", href=True):  # [file:98][m
[32m+[m[32m        if not _visible_tag(a):  # [file:98][m
[32m+[m[32m            continue  # [file:98][m
[32m+[m[32m        href = a.get("href","").strip()  # [file:98][m
[32m+[m[32m        if any(dom in href for dom in SOCIAL_DOMAINS):  # [file:98][m
[32m+[m[32m            label = a.get_text(" ", strip=True)  # [file:98][m
[32m+[m[32m            abs_url = urljoin(base_url, href)  # [file:98][m
[32m+[m[32m            socials.append({"href": abs_url, "label": label})  # [file:98][m
[32m+[m[32m            notes.append(f"social:{abs_url}|label:{label[:50]}")  # [file:98][m
[32m+[m[32m    return socials, notes  # [file:98][m
[32m+[m
[32m+[m[32mdef _extract_emails(soup, vis_texts):  # [file:98][m
[32m+[m[32m    emails, notes = set(), []  # [file:98][m
[32m+[m[32m    for a in soup.find_all("a", href=True):  # [file:98][m
[32m+[m[32m        if not _visible_tag(a):  # [file:98][m
[32m+[m[32m            continue  # [file:98][m
[32m+[m[32m        href = a["href"]  # [file:98][m
[32m+[m[32m        m = re.search(r"mailto:" + EMAIL_REGEX, href, re.IGNORECASE)  # [file:98][m
[32m+[m[32m        if m:  # [file:98][m
[32m+[m[32m            emails.add(m.group(1).strip().lower())  # [file:98][m
[32m+[m[32m            notes.append(f"mailto:{href}")  # [file:98][m
[32m+[m[32m    for t in vis_texts:  # [file:98][m
[32m+[m[32m        for m in re.finditer(EMAIL_REGEX, t, re.IGNORECASE):  # [file:98][m
[32m+[m[32m            emails.add(m.group(1).strip().lower())  # [file:98][m
[32m+[m[32m    return sorted(emails), notes  # [file:98][m
[32m+[m
[32m+[m[32mdef _extract_phones(soup, vis_texts):  # [file:98][m
[32m+[m[32m    phones, notes = set(), []  # [file:98][m
[32m+[m[32m    for a in soup.find_all("a", href=True):  # [file:98][m
[32m+[m[32m        if not _visible_tag(a):  # [file:98][m
[32m+[m[32m            continue  # [file:98][m
[32m+[m[32m        href = a["href"]  # [file:98][m
[32m+[m[32m        m = re.search(r"tel:" + PHONE_REGEX, href)  # [file:98][m
[32m+[m[32m        if m:  # [file:98][m
[32m+[m[32m            orig = m.group(1)  # [file:98][m
[32m+[m[32m            phones.add((orig.strip(), normalize_phone(orig)))  # [file:98][m
[32m+[m[32m            notes.append(f"tel:{href}")  # [file:98][m
[32m+[m[32m    for t in vis_texts:  # [file:98][m
[32m+[m[32m        for m in re.finditer(PHONE_REGEX, t):  # [file:98][m
[32m+[m[32m            orig = m.group(1)  # [file:98][m
[32m+[m[32m            phones.add((orig.strip(), normalize_phone(orig)))  # [file:98][m
[32m+[m[32m    out = [{"original": o, "normalized": n} for (o, n) in sorted(phones)]  # [file:98][m
[32m+[m[32m    seen, dedup = set(), []  # [file:98][m
[32m+[m[32m    for item in out:  # [file:98][m
[32m+[m[32m        key = item.get("normalized") or item.get("original")  # [file:98][m
[32m+[m[32m        if key and key not in seen:  # [file:98][m
[32m+[m[32m            seen.add(key)  # [file:98][m
[32m+[m[32m            dedup.append(item)  # [file:98][m
[32m+[m[32m    return dedup, notes  # [file:98][m
[32m+[m
[32m+[m[32mdef _title_case_name_candidates(texts):  # [file:98][m
[32m+[m[32m    names = set()  # [file:98][m
[32m+[m[32m    for t in texts:  # [file:98][m
[32m+[m[32m        for n in re.findall(r"\b([A-Z][a-z]+(?:\s+[A-Z][a-z]+){0,2})\b", t):  # [file:98][m
[32m+[m[32m            if len(n.split()) <= 3 and 3 <= len(n) <= 50:  # [file:98][m
[32m+[m[32m                names.add(n.strip())  # [file:98][m
[32m+[m[32m    return names  # [file:98][m
[32m+[m
[32m+[m[32mdef _email_hint_name(local_part: str):  # [file:98][m
[32m+[m[32m    tokens = re.split(r"[._\-]+", local_part)  # [file:98][m
[32m+[m[32m    tokens = [t for t in tokens if t and not t.isdigit() and len(t) >= 2]  # [file:98][m
[32m+[m[32m    if not tokens or len(tokens) > 3:  # [file:98][m
[32m+[m[32m        return None  # [file:98][m
[32m+[m[32m    return " ".join(w.capitalize() for w in tokens)  # [file:98][m
[32m+[m
[32m+[m[32mdef _score_name(name: str, context: str):  # [file:98][m
[32m+[m[32m    score = 0.5  # [file:98][m
[32m+[m[32m    low = context.lower()  # [file:98][m
[32m+[m[32m    for k in ("contact", "founder", "director", "owner", "ceo", "principal"):  # [file:98][m
[32m+[m[32m        if k in low:  # [file:98][m
[32m+[m[32m            score += 0.1  # [file:98][m
[32m+[m[32m    if len(name.split()) == 2:  # [file:98][m
[32m+[m[32m        score += 0.05  # [file:98][m
[32m+[m[32m    return min(score, 0.95)  # [file:98][m
[32m+[m
[32m+[m[32mdef _extract_name_candidates(soup, vis_texts, emails):  # [file:98][m
[32m+[m[32m    likely_blocks = []  # [file:98][m
[32m+[m[32m    for sel in ["h1","h2","h3",".author",".person",".profile",".about",".contact"]:  # [file:98][m
[32m+[m[32m        likely_blocks += soup.select(sel)  # [file:98][m
[32m+[m[32m    texts = [x.get_text(" ", strip=True) for x in likely_blocks if _visible_tag(x)]  # [file:98][m
[32m+[m[32m    texts += list(vis_texts)  # [file:98][m
[32m+[m[32m    title_case = set(_title_case_name_candidates(texts))  # [file:98][m
[32m+[m[32m    for e in emails:  # [file:98][m
[32m+[m[32m        local = e.split("@",1)[0]  # [file:98][m
[32m+[m[32m        hint = _email_hint_name(local)  # [file:98][m
[32m+[m[32m        if hint:  # [file:98][m
[32m+[m[32m            title_case.add(hint)  # [file:98][m
[32m+[m[32m    nearby = " ".join(texts[:200])[:2000]  # [file:98][m
[32m+[m[32m    scored = [{"name": n, "confidence": _score_name(n, nearby)} for n in title_case]  # [file:98][m
[32m+[m[32m    scored.sort(key=lambda x: x["confidence"], reverse=True)  # [file:98][m
[32m+[m[32m    seen, out = set(), []  # [file:98][m
[32m+[m[32m    for s in scored:  # [file:98][m
[32m+[m[32m        k = s["name"].lower()  # [file:98][m
[32m+[m[32m        if k in seen:  # [file:98][m
[32m+[m[32m            continue  # [file:98][m
[32m+[m[32m        seen.add(k)  # [file:98][m
[32m+[m[32m        out.append(s)  # [file:98][m
[32m+[m[32m        if len(out) >= 5:  # [file:98][m
[32m+[m[32m            break  # [file:98][m
[32m+[m[32m    return out  # [file:98][m
[32m+[m
[32m+[m[32mclass ContactsSpider(scrapy.Spider):  # [file:98][m
[32m+[m[32m    name = "contacts"  # [file:98][m
[32m+[m
[32m+[m[32m    def __init__(self, url=None, *args, **kwargs):  # [file:98][m
[32m+[m[32m        super().__init__(*args, **kwargs)  # [file:98][m
[32m+[m[32m        if not url:  # [file:98][m
[32m+[m[32m            raise ValueError("Pass -a url=<start_url>")  # [file:98][m
[32m+[m[32m        self.start_url = url  # [file:98][m
[32m+[m
[32m+[m[32m    async def start(self):  # [file:98][m
[32m+[m[32m        yield Request([m
[32m+[m[32m            self.start_url,[m
[32m+[m[32m            callback=self.parse_page,[m
[32m+[m[32m            errback=self.errback,[m
[32m+[m[32m            meta={[m
[32m+[m[32m                "playwright": True,[m
[32m+[m[32m                "playwright_include_page": True,[m
[32m+[m[32m                "playwright_page_goto_options": {"wait_until": "networkidle", "timeout": 90000},[m
[32m+[m[32m                "playwright_page_methods": [[m
[32m+[m[32m                    PageMethod("wait_for_selector", "body", state="attached", timeout=90000),[m
[32m+[m[32m                    PageMethod("wait_for_load_state", "networkidle"),[m
[32m+[m[32m                ],[m
[32m+[m[32m            },[m
[32m+[m[32m        )  # [file:98][m
[32m+[m
[32m+[m[32m    async def parse_page(self, response):  # [file:98][m
[32m+[m[32m        page = response.meta.get("playwright_page")  # [file:98][m
[32m+[m[32m        try:[m
[32m+[m[32m            soup = BeautifulSoup(response.text, "lxml")  # [file:98][m
[32m+[m[32m            vis_texts = list(_iter_visible_text(soup))  # [file:98][m
[32m+[m[32m            socials, s_notes = _extract_socials(soup, response.url)  # [file:98][m
[32m+[m[32m            emails, e_notes = _extract_emails(soup, vis_texts)  # [file:98][m
[32m+[m[32m            phones, p_notes = _extract_phones(soup, vis_texts)  # [file:98][m
[32m+[m[32m            name_cands = _extract_name_candidates(soup, vis_texts, emails)  # [file:98][m
[32m+[m
[32m+[m[32m            notes = {[m
[32m+[m[32m                "social_notes": s_notes[:50],[m
[32m+[m[32m              